Decision Trees Overview

Los arboles de decisión permiten construir modelos de regresión o clasificación.  Su nombre se atribuye a que, en esencia, la estructura se construye en forma de un árbol que contiene nodos de decisión y nodos hoja. Para construir un árbol de decisión, se pueden usar distintos algoritmos. Uno de ellos es el ID3, que mediante entropía y ganancia de información construye el árbol de decisión, además lo hace de arriba hacia abajo. Otro algoritmo es el índice de Gini, que, mediante una variable objetivo de tipo categórica, éxito o o error, calcula la suma de cuadrados de probabilidad para el éxito y el fracaso. Este algoritmo solo realiza divisiones binarias, cuanto mayor sea, mayor es la homogeneidad. El algoritmo de Chi-Cuadrado permite estimar el nivel de significancia de las diferencias entre los subnodos y el nodo padre, al igual que el índice de Gini, también funciona con la variable objetivo categórica éxito o error pero a diferencia de este, permite hacer mas de dos divisiones en un mismo nodo. Entre mas grande sea el valor de Chi-cuadrado, mayor será el nivel de significancia entre el subnodo y el nodo principal. Los algoritmos anteriormente explicados son aplicados para variables categóricas, el algoritmo de reducción de la variación se aplica a variables objetivo de tipo continuas (regresión).  Usa la formula de la desviación estándar como criterio de división de los nodos. En primera instancia se calcula la varianza para cada posible división y se escoge la que menor error represente. Dado que los arboles de decisión permiten resolver tanto problemas de regresión como de clasificación, tienen aplicaciones en múltiples campos de la industria, son fáciles de entender e interpretar y visualizar, y no son afectados por las relaciones no lineales entre parámetros. Sin embargo, es necesario tomar precauciones frente al overfitting y es necesario utilizar técnicas avanzadas de train y test para evitar sesgo.
